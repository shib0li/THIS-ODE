{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df082816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m(7180, 4)\u001b[0m\n",
      "\u001b[34m(4820, 4)\u001b[0m\n",
      "0.001 5.998271690654775\n",
      "6.002325613217111 10.000000000000002\n",
      "\u001b[31m(7180, 4)\u001b[0m\n",
      "\u001b[34m(4820, 4)\u001b[0m\n",
      "0.001 5.999285171295359\n",
      "6.002325613217111 10.000000000000002\n",
      "\u001b[31m(7223, 4)\u001b[0m\n",
      "\u001b[34m(4777, 4)\u001b[0m\n",
      "0.001 5.999285171295359\n",
      "6.002325613217111 10.000000000000002\n",
      "\u001b[31m(7145, 4)\u001b[0m\n",
      "\u001b[34m(4855, 4)\u001b[0m\n",
      "0.001 5.999285171295359\n",
      "6.002325613217111 10.000000000000002\n",
      "\u001b[31m(7164, 4)\u001b[0m\n",
      "\u001b[34m(4836, 4)\u001b[0m\n",
      "0.001 5.999285171295359\n",
      "6.002325613217111 10.000000000000002\n",
      "Directory 'processed' created successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm, trange\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "from infrastructure.randutils import *\n",
    "from infrastructure.misc import *\n",
    "# from utils import *\n",
    "\n",
    "# np.random.seed(0)\n",
    "\n",
    "def process_beijing_air(t_min, t_max, folds, max_samples, test_split, extrap):\n",
    "    \n",
    "    raw_path = os.path.join('raw', 'Beijing')\n",
    "    \n",
    "    train_idxs = np.load(os.path.join(raw_path,'train_idxs.npy'))\n",
    "    train_vals = np.load(os.path.join(raw_path,'train_vals.npy'))\n",
    "\n",
    "    test_idxs = np.load(os.path.join(raw_path,'test_idxs.npy'))\n",
    "    test_vals = np.load(os.path.join(raw_path,'test_vals.npy'))\n",
    "\n",
    "    train_idxs = train_idxs[:,[1,2,0]]\n",
    "    test_idxs = test_idxs[:,[1,2,0]]\n",
    "    \n",
    "#     cprint('r', train_idxs)\n",
    "#     cprint('r', test_idxs)\n",
    "\n",
    "    data = np.hstack([np.vstack([train_idxs, test_idxs]), np.concatenate([train_vals,test_vals]).reshape([-1,1])])\n",
    "      \n",
    "#     perm = generate_permutation_sequence(N=data.shape[0], seed=1)\n",
    "#     data = data[perm, :]\n",
    "    data = data[:max_samples, :]\n",
    "\n",
    "    \n",
    "    timestamp = data[:,-2].reshape([-1,1])\n",
    "    y = data[:,-1].reshape([-1,1])\n",
    "    \n",
    "    # normalize/scale time and observations\n",
    "    scaler_t = MinMaxScaler(feature_range=(t_min, t_max))\n",
    "    scaler_t.fit(timestamp)\n",
    "\n",
    "    timestamp_scaled = scaler_t.transform(timestamp)\n",
    "\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_y.fit(y)\n",
    "    \n",
    "    y_scaled = scaler_y.transform(y)\n",
    "    \n",
    "    data[:,-2] = timestamp_scaled.squeeze()\n",
    "    data[:,-1] = y_scaled.squeeze()\n",
    "    \n",
    "    kf = KFold(n_splits=folds)\n",
    "    \n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    for fold_idx, _ in kf.split(data):\n",
    "        \n",
    "        #cprint('r', data.shape)\n",
    "        \n",
    "        data_fold = data[fold_idx, :]\n",
    "        \n",
    "        sort_index = np.argsort(data_fold[:,-2])\n",
    "        data_fold = data_fold[sort_index, :]\n",
    "    \n",
    "#         t_split = t_max - test_split*t_max\n",
    "    \n",
    "#         data_tr = data_fold[data_fold[:,-2]<=t_split]\n",
    "#         data_te = data_fold[data_fold[:,-2]>t_split]\n",
    "\n",
    "        if extrap:\n",
    "            t_split = t_max - t_max*test_split\n",
    "            tr_idx = data_fold[:,-2]<=t_split\n",
    "            te_idx = data_fold[:,-2]>t_split\n",
    "        else:\n",
    "            t_split1 = (0.5-0.5*test_split)*t_max\n",
    "            t_split2 = (0.5+0.5*test_split)*t_max\n",
    "\n",
    "            #print(data[:,-2]<t_split1)\n",
    "            #print(data[:,-2]>=t_split2)\n",
    "            tr_idx = np.any([data_fold[:,-2]<t_split1, data_fold[:,-2]>=t_split2], axis=0)\n",
    "            te_idx = np.all([data_fold[:,-2]>=t_split1, data_fold[:,-2]<t_split2], axis=0)\n",
    "        #\n",
    "            \n",
    "        data_tr = data_fold[tr_idx]\n",
    "        data_te = data_fold[te_idx]\n",
    "    \n",
    "    \n",
    "        cprint('r', data_tr.shape)\n",
    "        cprint('b', data_te.shape)\n",
    "        \n",
    "#         cprint('r', np.unique(data_tr[:,0]))\n",
    "#         cprint('r', np.unique(data_tr[:,1]))\n",
    "        \n",
    "        print(data_tr[:,-2].min(), data_tr[:,-2].max())\n",
    "        print(data_te[:,-2].min(), data_te[:,-2].max())\n",
    "\n",
    "        train_list.append(data_tr)\n",
    "        test_list.append(data_te)\n",
    "    #\n",
    " \n",
    "    D = {}\n",
    "    D['nvec'] = [12, 6]\n",
    "    D['nmod'] = 2\n",
    "    D['train_folds'] = train_list\n",
    "    D['test_folds'] = test_list\n",
    "    D['t_min'] = t_min\n",
    "    D['t_max'] = t_max\n",
    "    \n",
    "    save_path = os.path.join('processed')\n",
    "    \n",
    "    if extrap:\n",
    "        pickle_name = 'BeijingAirExtrap' + '.pickle'\n",
    "    else:\n",
    "        pickle_name = 'BeijingAirInterp' + '.pickle'\n",
    "\n",
    "    create_path(save_path)\n",
    "\n",
    "    with open(os.path.join(save_path, pickle_name), 'wb') as handle:\n",
    "        pickle.dump(D, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    #\n",
    "\n",
    "Data = process_beijing_air(\n",
    "    t_min=0.001, \n",
    "    t_max=10.0, \n",
    "    folds=5, \n",
    "    max_samples=15000,\n",
    "    test_split=0.4,\n",
    "    extrap=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cff428d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a3bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
