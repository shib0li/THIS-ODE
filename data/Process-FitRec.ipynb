{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "186ba780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m(9867, 5)\u001b[0m\n",
      "\u001b[34m(5133, 5)\u001b[0m\n",
      "0.001 5.999296357615894\n",
      "6.0011357615894045 10.0\n",
      "\u001b[31m(10199, 5)\u001b[0m\n",
      "\u001b[34m(4801, 5)\u001b[0m\n",
      "0.001 5.999672667757775\n",
      "6.00149099836334 10.0\n",
      "\u001b[31m(10110, 5)\u001b[0m\n",
      "\u001b[34m(4890, 5)\u001b[0m\n",
      "0.001 5.9996646442360735\n",
      "6.001503033645892 10.0\n",
      "\u001b[31m(10226, 5)\u001b[0m\n",
      "\u001b[34m(4774, 5)\u001b[0m\n",
      "0.001 5.999307412128939\n",
      "6.001128391914042 10.0\n",
      "\u001b[31m(10060, 5)\u001b[0m\n",
      "\u001b[34m(4940, 5)\u001b[0m\n",
      "0.001 5.998215222141297\n",
      "6.000035870356884 10.0\n",
      "Directory 'processed' created successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm, trange\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "from infrastructure.randutils import *\n",
    "from infrastructure.misc import *\n",
    "# from utils import *\n",
    "\n",
    "# import gdown\n",
    "\n",
    "\n",
    "# def download_by_name(fname, output=None, quiet=False):\n",
    "#     url = 'https://drive.google.com/uc?id=12ymlWEcKhVuQ3syNb92zVMmowAsZwSZ4&export=download'\n",
    "#     gdown.download(url, output=fname, quiet=quiet)\n",
    "\n",
    "# raw_path = os.path.join('/workspace','data', 'raw', 'FitRec')\n",
    "# create_path(raw_path)\n",
    "# os.chdir(raw_path)\n",
    "\n",
    "# raw_fname = 'endomondoHR_proper.json'\n",
    "\n",
    "# download_by_name(fname=raw_fname, output=raw_path)\n",
    "\n",
    "def load_raw_fitrec(test_split):\n",
    "    \n",
    "    raw_path = os.path.join('raw', 'FitRec','endomondoHR_proper.json')\n",
    "    \n",
    "#     data_tr = []\n",
    "#     data_te = []\n",
    "\n",
    "    data = []\n",
    "    \n",
    "    unique_sports = {}\n",
    "    nunique_sports = 0\n",
    "    \n",
    "    steps = 0\n",
    "    \n",
    "    with open(raw_path) as f:\n",
    "        for l in tqdm(f):\n",
    "  \n",
    "            d = eval(l)\n",
    "            \n",
    "            if d['sport'] in unique_sports.keys():\n",
    "                sport_id = unique_sports[d['sport']]\n",
    "            else:\n",
    "                nunique_sports += 1\n",
    "                unique_sports[d['sport']] = nunique_sports\n",
    "            #\n",
    "            \n",
    "            timestamp = np.array(d['timestamp'])\n",
    "            heart_rate = np.array(d['heart_rate'])\n",
    "            altitude = np.array(d['altitude'])\n",
    "            \n",
    "            timestamp = timestamp - timestamp[0]\n",
    "\n",
    "            if timestamp[-1] < 4500 or timestamp[-1] > 5500:\n",
    "                continue\n",
    "            \n",
    "            assert timestamp.shape[0] == heart_rate.shape[0] == altitude.shape[0]\n",
    "            nsamples = timestamp.shape[0]\n",
    "            \n",
    "            array_id = np.repeat(d['id'], nsamples).reshape([-1,1])\n",
    "            array_sport = np.repeat(unique_sports[d['sport']], nsamples).reshape([-1,1])\n",
    "            \n",
    "            dij = np.hstack([\n",
    "                array_id, \n",
    "                array_sport, \n",
    "                altitude.reshape([-1,1]),\n",
    "                timestamp.reshape([-1,1]),\n",
    "                heart_rate.reshape([-1,1]),\n",
    "            ])\n",
    "\n",
    "            data.append(dij)\n",
    "   \n",
    "            steps += 1\n",
    "            if steps >= 1000:\n",
    "                break\n",
    "            #\n",
    "        #\n",
    "    #\n",
    "\n",
    "    data = np.vstack(data)\n",
    "        \n",
    "    df = pd.DataFrame(data, columns=['user_id','sport_id','altitude','timestamp','heart_rate'])\n",
    "    \n",
    "    save_path = os.path.join('raw', 'FitRec')\n",
    "    create_path(save_path)\n",
    "    \n",
    "    df.to_csv(os.path.join(save_path, 'heart_rate.csv.gz'), compression='gzip', index=False)\n",
    "#\n",
    "\n",
    "\n",
    "def process_fit_record(K_user, K_sport, K_alt, t_min, t_max, folds, max_samples, test_split, extrap):\n",
    "    \n",
    "    load_path = os.path.join('raw', 'FitRec', 'heart_rate.csv.gz')\n",
    "\n",
    "    df = pd.read_csv(load_path, compression='gzip')\n",
    "\n",
    "    #display(df)\n",
    "    \n",
    "    unique_users = df['user_id'].unique()\n",
    "    \n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    for fold in range(folds):\n",
    "        \n",
    "        #print(df.size)\n",
    "    \n",
    "        K_choice = generate_random_choice(a=unique_users.size, N=K_user, seed=fold)\n",
    "        \n",
    "        topK_user = unique_users[K_choice]\n",
    "        \n",
    "        #cprint('g', topK_user)\n",
    "\n",
    "        df_fold = df.loc[df['user_id'].isin(topK_user)]\n",
    "        \n",
    "        topK_sport = df_fold['sport_id'].value_counts().index[:K_sport]\n",
    "        \n",
    "        #cprint('g', topK_sport)\n",
    "        \n",
    "        #print(topK_sport)\n",
    "        \n",
    "        df_fold = df_fold.loc[df_fold['sport_id'].isin(topK_sport)]\n",
    "        \n",
    "        #print(df_fold['sport_id'].unique())\n",
    "        \n",
    "        #display(df_fold)\n",
    "        \n",
    "        df_fold.sport_id = pd.factorize(df_fold.sport_id)[0]\n",
    "        df_fold.user_id = pd.factorize(df_fold.user_id)[0]\n",
    "        \n",
    "        #display(df_fold)\n",
    "        \n",
    "        data = df_fold.to_numpy().astype(float)\n",
    "        \n",
    "        altitude = data[:,-3].reshape([-1,1])\n",
    "        #print(altitude.min())\n",
    "        #print(altitude.max())\n",
    "        \n",
    "        bins_altitude = np.linspace(\n",
    "            start=altitude.min(),\n",
    "            stop=altitude.max(),\n",
    "            num=K_alt+1\n",
    "        )[1:-1]\n",
    "        \n",
    "        b_altitude = np.digitize(altitude, bins=bins_altitude)\n",
    "        data[:,-3] = b_altitude.squeeze()\n",
    "        \n",
    "        timestamp = data[:,-2].reshape([-1,1])\n",
    "        y = data[:,-1].reshape([-1,1])\n",
    "        \n",
    "        # normalize/scale time and observations\n",
    "        scaler_t = MinMaxScaler(feature_range=(t_min, t_max))\n",
    "        scaler_t.fit(timestamp)\n",
    "    \n",
    "        scaler_y = StandardScaler()\n",
    "        scaler_y.fit(y)\n",
    "        \n",
    "        scaled_time = scaler_t.transform(timestamp)\n",
    "        scaled_y = scaler_y.transform(y)\n",
    "        \n",
    "        data[:,-2] = scaled_time.squeeze()\n",
    "        data[:,-1] = scaled_y.squeeze()\n",
    "        \n",
    "#         t_split = t_max * (1-test_split)\n",
    "        \n",
    "#         idx_tr = scaled_time.squeeze() <= t_split\n",
    "#         idx_te = scaled_time.squeeze() > t_split\n",
    "        \n",
    "#         data_tr = data[idx_tr, :]\n",
    "#         data_te = data[idx_te, :]\n",
    "\n",
    "        if extrap:\n",
    "            t_split = t_max - t_max*test_split\n",
    "            tr_idx = data[:,-2]<=t_split\n",
    "            te_idx = data[:,-2]>t_split\n",
    "        else:\n",
    "            t_split1 = (0.5-0.5*test_split)*t_max\n",
    "            t_split2 = (0.5+0.5*test_split)*t_max\n",
    "\n",
    "            #print(data[:,-2]<t_split1)\n",
    "            #print(data[:,-2]>=t_split2)\n",
    "            tr_idx = np.any([data[:,-2]<t_split1, data[:,-2]>=t_split2], axis=0)\n",
    "            te_idx = np.all([data[:,-2]>=t_split1, data[:,-2]<t_split2], axis=0)\n",
    "        #\n",
    "        \n",
    "        data_tr = data[tr_idx]\n",
    "        data_te = data[te_idx]\n",
    "        \n",
    "        cprint('r', data_tr.shape)\n",
    "        cprint('b', data_te.shape)\n",
    "        \n",
    "        print(data_tr[:,-2].min(), data_tr[:,-2].max())\n",
    "        print(data_te[:,-2].min(), data_te[:,-2].max())\n",
    "        \n",
    "        train_list.append(data_tr)\n",
    "        test_list.append(data_te)\n",
    "        \n",
    "    #\n",
    "    \n",
    "    D = {}\n",
    "    D['nvec'] = [K_user, K_sport, K_alt]\n",
    "    D['nmod'] = 3\n",
    "    D['train_folds'] = train_list\n",
    "    D['test_folds'] = test_list\n",
    "    D['t_min'] = t_min\n",
    "    D['t_max'] = t_max\n",
    "    \n",
    "    save_path = os.path.join('processed')\n",
    "#     pickle_name = 'FitRec' + '.pickle'\n",
    "\n",
    "    if extrap:\n",
    "        pickle_name = 'FitRecExtrap' + '.pickle'\n",
    "    else:\n",
    "        pickle_name = 'FitRecInterp' + '.pickle'\n",
    "\n",
    "    create_path(save_path)\n",
    "\n",
    "    with open(os.path.join(save_path, pickle_name), 'wb') as handle:\n",
    "        pickle.dump(D, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    #\n",
    "\n",
    "Data = process_fit_record(\n",
    "    K_user=30,\n",
    "    K_sport=20,\n",
    "    K_alt=20,\n",
    "    t_min=0.001, \n",
    "    t_max=10.0, \n",
    "    folds=5, \n",
    "    max_samples=12000,\n",
    "    test_split=0.4,\n",
    "    extrap=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f767b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4f64ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
